{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Finance\n",
    "\n",
    "**Chapter 01 &mdash; Learning through Interaction**\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe3719-fcab-4963-8701-087562dd5d79",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a93b6",
   "metadata": {},
   "source": [
    "This notebook demonstrates fundamental concepts of **learning through interaction** - a core principle of reinforcement learning. We'll explore how an agent can learn optimal strategies through trial and error when interacting with an environment.\n",
    "\n",
    "The examples progress from simple random choices to more sophisticated learning strategies that adapt based on observed outcomes. This mimics how reinforcement learning agents improve their decision-making over time.\n",
    "\n",
    "**Key Learning Objectives:**\n",
    "- Understand the difference between random action selection and adaptive learning\n",
    "- See how agents can discover patterns in biased environments\n",
    "- Learn about exploration vs exploitation trade-offs\n",
    "- Observe how simple frequency-based learning can improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8710a-19a3-4b5b-ae60-ffbd53dc45c4",
   "metadata": {},
   "source": [
    "### Tossing a Biased Coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d781056-299a-4dd5-8908-038a2438ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644cc3e",
   "metadata": {},
   "source": [
    "### Setting Up the Environment\n",
    "\n",
    "First, we'll set up our random number generator with a fixed seed for reproducibility. This ensures that our experiments will give consistent results across different runs, making it easier to understand the learning patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf14cf9-53ed-428b-a597-3f380f4cff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2b1719-cac4-46c2-9398-c634068d3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "asp = [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34635fe",
   "metadata": {},
   "source": [
    "### Experiment 1: Fair Coin with Random Actions\n",
    "\n",
    "Let's start with the simplest case - a fair coin toss where:\n",
    "- `ssp = [1, 0]` represents the environment's state space (the coin can show heads=1 or tails=0)\n",
    "- `asp = [1, 0]` represents the agent's action space (the agent can predict heads=1 or tails=0)\n",
    "\n",
    "In this first experiment, both the coin and the agent's choices are completely random and fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cfd83a-7e57-4fa4-8126-81bb7a4758ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch():\n",
    "    tr = 0\n",
    "    for _ in range(100):\n",
    "        a = rng.choice(asp)\n",
    "        s = rng.choice(ssp)\n",
    "        if a == s:\n",
    "            tr += 1\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffb04e",
   "metadata": {},
   "source": [
    "The `epoch()` function simulates one episode of interaction:\n",
    "- We run 100 trials per epoch\n",
    "- In each trial, the agent chooses an action `a` randomly from `asp`\n",
    "- The environment reveals its state `s` randomly from `ssp`\n",
    "- If the agent's prediction matches the environment's outcome (`a == s`), we count it as a success\n",
    "- The function returns the total number of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fc1f62-24df-4baa-9784-3126431dbdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56, 47, 48, 55, 55, 51, 54, 43, 55, 40])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = np.array([epoch() for _ in range(250)])\n",
    "rl[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26ada6",
   "metadata": {},
   "source": [
    "Now let's run 250 epochs to see how well the agent performs with random choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f92a52-d305-42f1-a1ff-7b3aacc26549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.968"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4fefe",
   "metadata": {},
   "source": [
    "**Expected Result:** With both the coin and agent choices being random and fair, we expect about 50% success rate (around 50 correct predictions out of 100 trials per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855b4cf5-75d8-4dbc-bdae-1cd753e50691",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = [1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8148d8b9-de41-4d16-ab8f-b41d45a2a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "asp = [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f333c",
   "metadata": {},
   "source": [
    "### Experiment 2: Biased Coin with Random Actions\n",
    "\n",
    "Now we introduce bias into the environment:\n",
    "- `ssp = [1, 1, 1, 1, 0]` means the coin shows heads (1) with 80% probability and tails (0) with 20% probability\n",
    "- `asp = [1, 0]` the agent still chooses randomly with 50/50 probability\n",
    "\n",
    "This demonstrates what happens when there's a pattern in the environment but the agent doesn't adapt to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea9ad54-804a-4d76-a614-50b01be65805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch():\n",
    "    tr = 0\n",
    "    for _ in range(100):\n",
    "        a = rng.choice(asp)\n",
    "        s = rng.choice(ssp)\n",
    "        if a == s:\n",
    "            tr += 1\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988094e8-64c7-46e4-a54e-f111765c9e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 56, 40, 55, 53, 49, 43, 45, 50, 51])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = np.array([epoch() for _ in range(250)])\n",
    "rl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aeed633-c81c-4b7f-9e19-c1a03ac3e32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.924"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e3edf",
   "metadata": {},
   "source": [
    "**Result Analysis:** Even though the environment is biased toward heads (80% probability), the agent still performs randomly at ~50% success rate because it doesn't learn from the pattern. The optimal strategy would be to always predict heads (1), which would yield 80% success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2220ff9-c8c2-462f-aad0-c07405272976",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = [1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e043cb3e-b943-4c4a-a337-f50810795d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(n):\n",
    "    tr = 0\n",
    "    asp = [0, 1]\n",
    "    for _ in range(n):\n",
    "        a = rng.choice(asp)\n",
    "        s = rng.choice(ssp)\n",
    "        if a == s:\n",
    "            tr += 1\n",
    "        asp.append(s)\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3779299",
   "metadata": {},
   "source": [
    "### Experiment 3: Biased Coin with Simple Learning\n",
    "\n",
    "Now we introduce a simple learning mechanism. The agent still starts with random actions but:\n",
    "- **Learning mechanism**: After each trial, the agent adds the observed outcome to its action space\n",
    "- **Action selection**: The agent still chooses randomly, but now the action space contains more copies of frequently observed outcomes\n",
    "\n",
    "This is a form of **experience replay** where past observations influence future decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63ed3ba7-5701-4613-8a37-94eb4b114354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80, 62, 73, 67, 74, 63, 70, 62, 63, 65])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = np.array([epoch(100) for _ in range(250)])\n",
    "rl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb173db-cf9f-4ee2-8bb1-f2b41990f130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.66"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ba4ce",
   "metadata": {},
   "source": [
    "**Result Analysis:** The performance improves significantly! By adding observed outcomes to the action space, the agent naturally gravitates toward predicting the more frequent outcome (heads). This simple learning mechanism shows how agents can adapt to environmental patterns without explicit programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74d45682-4f46-4950-b35c-2f8dff86d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "535ead89-8667-48ae-830f-ec6679780272",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = [1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67569ec3-4525-443e-8cda-390af539804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(n):\n",
    "    tr = 0\n",
    "    asp = [0, 1]\n",
    "    for _ in range(n):\n",
    "        c = Counter(asp)\n",
    "        a = c.most_common()[0][0]\n",
    "        s = rng.choice(ssp)\n",
    "        if a == s:\n",
    "            tr += 1\n",
    "        asp.append(s)\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013192b0",
   "metadata": {},
   "source": [
    "### Experiment 4: Biased Coin with Optimal Strategy\n",
    "\n",
    "Now we implement a more sophisticated learning strategy:\n",
    "- **Frequency tracking**: We use `Counter` to track the frequency of observed outcomes\n",
    "- **Greedy action selection**: The agent always chooses the action that has been most frequently observed (`most_common()[0][0]`)\n",
    "\n",
    "This represents a **greedy exploitation** strategy where the agent always chooses the currently best-known action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc5893e5-a997-4fe8-88a4-13afe44c5175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([78, 79, 77, 80, 73, 79, 79, 79, 82, 84])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = np.array([epoch(100) for _ in range(250)])\n",
    "rl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7187f48e-e276-4f0a-959b-62ddc1bd23e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.264"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66427c3",
   "metadata": {},
   "source": [
    "**Result Analysis:** This greedy strategy performs even better! The agent quickly identifies the most frequent outcome and exploits it consistently. However, this approach has limitations - it doesn't explore alternative actions once it finds a good one, which could be problematic if the environment changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451350fe-1075-4969-808c-b5aaf37cec25",
   "metadata": {},
   "source": [
    "### Rolling a Biased Die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf4b0649-b1fa-4b74-bd31-ae5f20d00105",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = [1, 2, 3, 4, 4, 4, 4, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e3900fe-b22b-4ea2-b00c-8d057e553cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "asp = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707feea",
   "metadata": {},
   "source": [
    "### Experiment 5: Biased Die with Random Actions\n",
    "\n",
    "Now let's extend our concepts to a more complex scenario - a biased six-sided die:\n",
    "- `ssp = [1, 2, 3, 4, 4, 4, 4, 4, 5, 6]` means the die is heavily biased toward rolling 4 (50% probability)\n",
    "- `asp = [1, 2, 3, 4, 5, 6]` the agent chooses randomly from all six options\n",
    "\n",
    "This demonstrates the same concepts but with more possible outcomes, making the learning challenge more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bac0a45-5a2a-4276-a329-86978e3f9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch():\n",
    "    tr = 0\n",
    "    for _ in range(600):\n",
    "        a = rng.choice(asp)\n",
    "        s = rng.choice(ssp)\n",
    "        if a == s:\n",
    "            tr += 1\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "062abdd3-2a65-4d1e-a9af-cf25772b54c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([128,  89, 110,  93,  97,  88, 103,  93, 101, 102])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = np.array([epoch() for _ in range(250)])\n",
    "rl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a82b6f5f-7b32-403a-94a5-91ebc9e90815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.236"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd824d",
   "metadata": {},
   "source": [
    "**Random Strategy Result:** With 6 possible outcomes and random selection, we expect approximately 16.7% success rate (1/6). However, since the die is biased toward 4 (50% probability), random guessing will perform slightly better when the agent happens to guess 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e3a9fb0-22ea-4fed-8ff3-f0ab48169031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch():\n",
    "    tr = 0\n",
    "    asp = [1, 2, 3, 4, 5, 6]\n",
    "    for _ in range(600):\n",
    "        a = rng.choice(asp)\n",
    "        s = rng.choice(ssp)\n",
    "        if a == s:\n",
    "            tr += 1\n",
    "        asp.append(s)\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb98fa8",
   "metadata": {},
   "source": [
    "### Experiment 6: Biased Die with Simple Learning\n",
    "\n",
    "Applying the same simple learning strategy to the die scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79f099b7-ca59-45d1-bb10-0f19c8f7fd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([186, 193, 165, 163, 191, 192, 165, 160, 166, 190])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = np.array([epoch() for _ in range(250)])\n",
    "rl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd641f5f-205e-4414-8006-1a8464aa49cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177.044"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be27b1cb-19bf-4c08-bffe-84e7164a2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch():\n",
    "    tr = 0\n",
    "    asp = [1, 2, 3, 4, 5, 6]\n",
    "    for _ in range(600):\n",
    "        c = Counter(asp)\n",
    "        a = c.most_common()[0][0]\n",
    "        s = rng.choice(ssp)\n",
    "        if a == s:\n",
    "            tr += 1\n",
    "        asp.append(s)\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5df599",
   "metadata": {},
   "source": [
    "### Experiment 7: Biased Die with Greedy Strategy\n",
    "\n",
    "Finally, let's apply the greedy strategy (always choosing the most common observed outcome) to the die scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd68ba52-aaca-4c17-819e-5a9f96053c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([274, 303, 314, 309, 298, 298, 301, 305, 296, 293])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = np.array([epoch() for _ in range(250)])\n",
    "rl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c6b0239-493d-4fc8-8ca9-2dd49f8eff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298.9"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aaf45a",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "This notebook demonstrated fundamental concepts of reinforcement learning through simple experiments:\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Random vs. Learning**: Random action selection performs at baseline levels regardless of environmental patterns\n",
    "2. **Adaptation**: Simple learning mechanisms can significantly improve performance by adapting to environmental biases\n",
    "3. **Exploitation vs. Exploration**: Greedy strategies excel at exploiting known patterns but may miss better alternatives\n",
    "4. **Scalability**: The same learning principles apply whether dealing with simple (2-outcome) or complex (6-outcome) environments\n",
    "\n",
    "### Reinforcement Learning Connections:\n",
    "\n",
    "- **Agent**: The prediction mechanism that chooses actions\n",
    "- **Environment**: The coin/die that provides states and rewards\n",
    "- **Action Space**: The possible predictions (heads/tails or 1-6)\n",
    "- **State Space**: The possible outcomes from the environment\n",
    "- **Reward**: Success when prediction matches outcome\n",
    "- **Policy**: The strategy for choosing actions (random, learned, greedy)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "These concepts form the foundation for more sophisticated RL algorithms like Q-learning, policy gradients, and actor-critic methods that we'll explore in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0337cd1-b618-48df-bb51-3686caa3f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,000,000,000,000,000,000,000,000,000,000,000,000,000\n"
     ]
    }
   ],
   "source": [
    "cm = 10 ** 40\n",
    "print(f'{cm:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
