{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Finance\n",
    "\n",
    "**Chapter 03 &mdash; Financial Q-Learning**\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be6f8b-e00e-402c-9df1-1d3f16e76c7e",
   "metadata": {},
   "source": [
    "## Finance Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3188d",
   "metadata": {},
   "source": [
    "## Introduction to Financial Q-Learning\n",
    "\n",
    "This notebook represents a major milestone: **applying Deep Q-Learning to real financial data**. We transition from the controlled CartPole environment to the complex, noisy world of financial markets.\n",
    "\n",
    "### Key Learning Objectives:\n",
    "- **Custom Environment Design**: Creating a financial prediction environment from scratch\n",
    "- **Real Data Integration**: Working with actual EUR/USD exchange rate data\n",
    "- **Financial Feature Engineering**: Using price levels vs. returns as state representations\n",
    "- **Performance Metrics**: Adapting RL success criteria for financial prediction accuracy\n",
    "- **Risk Management**: Implementing early stopping based on performance thresholds\n",
    "\n",
    "### The Challenge:\n",
    "Financial markets are fundamentally different from game environments:\n",
    "- **Noisy signals**: Market data contains significant random variation\n",
    "- **Non-stationary**: Market patterns change over time\n",
    "- **Sparse rewards**: Good predictions may not be immediately rewarded\n",
    "- **Real consequences**: Mistakes have financial impact\n",
    "\n",
    "### Our Approach:\n",
    "We'll create a custom `Finance` environment that:\n",
    "1. **Loads real market data** (EUR/USD exchange rates)\n",
    "2. **Defines prediction tasks** (will price go up or down?)\n",
    "3. **Provides meaningful rewards** (accuracy-based scoring)\n",
    "4. **Implements risk controls** (stops trading if accuracy drops too low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c8cd7e-d93d-4c4d-ba77-3c0cb7b677af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8d3cf4-c30c-432a-bd3f-23e98c4d201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "os.environ['PYTHONHASHSEED'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249c8a6",
   "metadata": {},
   "source": [
    "### Environment Setup and Reproducibility\n",
    "\n",
    "Setting up deterministic behavior for consistent results across runs - crucial when working with financial data where small changes can lead to very different outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb33cd0c-4fb1-4456-911f-0d92597db8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionSpace:\n",
    "    def sample(self):\n",
    "        return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d49bdd-e24b-4d87-a4dc-5639cc172f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = ActionSpace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06124d27",
   "metadata": {},
   "source": [
    "### Custom Action Space for Financial Predictions\n",
    "\n",
    "Unlike Gymnasium's built-in environments, we need to create our own action space for financial prediction:\n",
    "\n",
    "**ActionSpace Class:**\n",
    "- **Action 0**: Predict price will go DOWN (bearish)\n",
    "- **Action 1**: Predict price will go UP (bullish)\n",
    "- **`sample()`**: Randomly selects between the two actions\n",
    "\n",
    "This binary prediction task simplifies the complex world of trading into a fundamental question: \"Will the asset price increase or decrease in the next period?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416ce315-16d7-4c47-845a-f21a099b8ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[action_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4df457f-9014-4e6a-878a-23645c77037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952353e1-8f39-48ac-ac6d-5a21b9a44315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finance:\n",
    "    url = 'https://certificate.tpq.io/rl4finance.csv'\n",
    "    def __init__(self, symbol, feature,\n",
    "                 min_accuracy=0.485, n_features=4):\n",
    "        self.symbol = symbol\n",
    "        self.feature = feature\n",
    "        self.n_features = n_features\n",
    "        self.action_space = ActionSpace()\n",
    "        self.min_accuracy = min_accuracy\n",
    "        self._get_data()\n",
    "        self._prepare_data()\n",
    "    def _get_data(self):\n",
    "        self.raw = pd.read_csv(self.url,\n",
    "                index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df360cbc",
   "metadata": {},
   "source": [
    "### The Finance Environment Class\n",
    "\n",
    "This is the heart of our financial RL system. The `Finance` class creates a custom Gymnasium-like environment for financial prediction:\n",
    "\n",
    "**Key Parameters:**\n",
    "- **symbol**: The financial instrument to trade (e.g., 'EUR=' for EUR/USD)\n",
    "- **feature**: What to use as state ('EUR=' for prices, 'r' for returns)\n",
    "- **min_accuracy**: Minimum prediction accuracy before stopping (risk control)\n",
    "- **n_features**: Number of historical periods to use as state (default: 4)\n",
    "\n",
    "**Data Pipeline:**\n",
    "1. **Download**: Fetches real market data from online source\n",
    "2. **Process**: Calculates returns and direction labels\n",
    "3. **Normalize**: Standardizes features for neural network training\n",
    "4. **Structure**: Creates sequences for time-series prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e1ed75-1e55-42f4-86a3-db54c60acf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finance(Finance):\n",
    "    def _prepare_data(self):\n",
    "        self.data = pd.DataFrame(self.raw[self.symbol]).dropna()\n",
    "        self.data['r'] = np.log(self.data / self.data.shift(1))\n",
    "        self.data['d'] = np.where(self.data['r'] > 0, 1, 0)\n",
    "        self.data.dropna(inplace=True)\n",
    "        self.data_ = (self.data - self.data.mean()) / self.data.std()\n",
    "    def reset(self):\n",
    "        self.bar = self.n_features\n",
    "        self.treward = 0\n",
    "        state = self.data_[self.feature].iloc[\n",
    "            self.bar - self.n_features:self.bar].values\n",
    "        return state, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e50b2e",
   "metadata": {},
   "source": [
    "### Data Preparation and Episode Initialization\n",
    "\n",
    "**`_prepare_data()` Method:**\n",
    "- **Return calculation**: `np.log(price[t] / price[t-1])` computes logarithmic returns\n",
    "- **Direction labeling**: `d = 1` if return > 0 (price up), `d = 0` if return â‰¤ 0 (price down)\n",
    "- **Normalization**: Standardizes features to have mean=0, std=1 (crucial for neural networks)\n",
    "\n",
    "**`reset()` Method:**\n",
    "- **Episode start**: Initializes a new trading episode\n",
    "- **State construction**: Returns the last `n_features` normalized values as initial state\n",
    "- **Progress tracking**: Resets reward accumulation and position tracking\n",
    "\n",
    "**Why logarithmic returns?**\n",
    "- **Statistical properties**: Log returns are more normally distributed\n",
    "- **Additive**: Easy to aggregate over time periods\n",
    "- **Scale invariant**: Works across different price levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b0ccc6-d8ec-4156-bf7a-30ba263fdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finance(Finance):\n",
    "    def step(self, action):\n",
    "        if action == self.data['d'].iloc[self.bar]:\n",
    "            correct = True\n",
    "        else:\n",
    "            correct = False\n",
    "        reward = 1 if correct else 0\n",
    "        self.treward += reward\n",
    "        self.bar += 1\n",
    "        self.accuracy = self.treward / (self.bar - self.n_features)\n",
    "        if self.bar >= len(self.data):\n",
    "            done = True\n",
    "        elif reward == 1:\n",
    "            done = False\n",
    "        elif (self.accuracy < self.min_accuracy) and (self.bar > 15):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        next_state = self.data_[self.feature].iloc[\n",
    "            self.bar - self.n_features:self.bar].values\n",
    "        return next_state, reward, done, False, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ce79d",
   "metadata": {},
   "source": [
    "### The Step Method: Core Trading Logic\n",
    "\n",
    "**`step(action)` Method Implementation:**\n",
    "\n",
    "**1. Prediction Checking:**\n",
    "- Compares agent's action with actual market direction (`data['d'].iloc[self.bar]`)\n",
    "- Binary reward: 1 for correct prediction, 0 for incorrect\n",
    "\n",
    "**2. Performance Tracking:**\n",
    "- **`treward`**: Total correct predictions in current episode\n",
    "- **`accuracy`**: Running accuracy percentage (treward / total_predictions)\n",
    "\n",
    "**3. Episode Termination Logic:**\n",
    "- **Natural end**: Reached end of data\n",
    "- **Success continuation**: Correct predictions allow episode to continue\n",
    "- **Risk control**: Episode ends if accuracy drops below `min_accuracy` threshold after 15+ predictions\n",
    "\n",
    "**4. State Progression:**\n",
    "- **Time advancement**: Move to next market period\n",
    "- **State update**: Return next `n_features` values as new state\n",
    "\n",
    "**Financial Interpretation:**\n",
    "This simulates a trading system that stops when performance degrades, protecting capital from further losses - a crucial risk management principle in quantitative trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "373a0a8c-3b85-4933-8de5-1103d4cc1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = Finance(symbol='EUR=', feature='EUR=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4c4248b-2168-42d2-b766-27270681b5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL.O',\n",
       " 'MSFT.O',\n",
       " 'INTC.O',\n",
       " 'AMZN.O',\n",
       " 'GS.N',\n",
       " '.SPX',\n",
       " '.VIX',\n",
       " 'SPY',\n",
       " 'EUR=',\n",
       " 'XAU=',\n",
       " 'GDX',\n",
       " 'GLD']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fin.raw.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea7819",
   "metadata": {},
   "source": [
    "### Exploring the Financial Dataset\n",
    "\n",
    "Let's examine what financial instruments are available in our dataset. This real-world dataset contains various currency pairs, indices, and commodities - providing a rich environment for testing trading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c2042dd-3d9a-4976-bb6d-d58daeeaf650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.74844931, 2.64643904, 2.69560062, 2.68085214]), {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin.reset()\n",
    "# four lagged, normalized price points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d40cf",
   "metadata": {},
   "source": [
    "### State Representation: Price Levels\n",
    "\n",
    "**Using 'EUR=' as feature means our state consists of:**\n",
    "- Four consecutive normalized EUR/USD price levels\n",
    "- Each value represents the standardized price at different time points\n",
    "- The agent must learn patterns in price movements to predict future direction\n",
    "\n",
    "**Example state interpretation:**\n",
    "- Values closer to 0: Prices near historical average\n",
    "- Positive values: Prices above historical average  \n",
    "- Negative values: Prices below historical average\n",
    "- Sequence patterns: Trends, reversals, momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0e04a87-7f63-4532-8609-2ad598d67067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6a11b6-87da-4226-baad-0fa9f4942c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.64643904, 2.69560062, 2.68085214, 2.63046153]), 0, False, False, {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin.step(fin.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad542f2",
   "metadata": {},
   "source": [
    "**Testing Environment Interaction:**\n",
    "- **Action**: Random prediction (0 or 1)\n",
    "- **Returns**: (next_state, reward, done, truncated, info)\n",
    "- **Reward**: 1 if prediction correct, 0 if incorrect\n",
    "- **State transition**: Next 4 normalized values in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a3b905-2eea-406f-9bee-bb61d6f5e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = Finance('EUR=', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c490647f-9757-46bf-911d-c53477d9b3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.19130476, -1.21344494,  0.61099805, -0.16094865]), {})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin.reset()\n",
    "# four lagged, normalized log returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63fbe2",
   "metadata": {},
   "source": [
    "### Alternative State Representation: Returns\n",
    "\n",
    "**Using 'r' (returns) as feature provides different information:**\n",
    "- Four consecutive normalized logarithmic returns\n",
    "- Each value represents the price change (percentage) between periods\n",
    "- Often more stationary than price levels (better for ML)\n",
    "- Focuses on momentum and volatility rather than absolute price levels\n",
    "\n",
    "**Returns vs. Prices:**\n",
    "- **Returns**: \"How much did it move?\" - captures dynamics\n",
    "- **Prices**: \"Where is it now?\" - captures levels and trends\n",
    "- Different features may lead to different trading strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c0bab87-6d45-4e17-a52c-3d19273bd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.env = Finance('EUR=', 'r')\n",
    "    def play(self, episodes=1):\n",
    "        self.trewards = list()\n",
    "        for e in range(episodes):\n",
    "            self.env.reset()\n",
    "            for step in range(1, 100):\n",
    "                a = self.env.action_space.sample()\n",
    "                state, reward, done, trunc, info = self.env.step(a)\n",
    "                if done:\n",
    "                    self.trewards.append(step)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060b7be",
   "metadata": {},
   "source": [
    "### Financial Random Agent Baseline\n",
    "\n",
    "The `RandomAgent` for financial prediction serves the same purpose as in CartPole - establishing a baseline performance level. However, the interpretation is different:\n",
    "\n",
    "**Financial Context:**\n",
    "- **Expected performance**: ~50% accuracy (random guessing on binary prediction)\n",
    "- **Episode length**: How many predictions before stopping (due to poor performance)\n",
    "- **Market efficiency**: If random performs well, market may be very efficient (hard to predict)\n",
    "\n",
    "**Why this matters:**\n",
    "- **Benchmark**: Any learning algorithm must beat random performance\n",
    "- **Market reality check**: Many professional traders struggle to beat random selection\n",
    "- **Risk assessment**: Shows how quickly poor strategies get stopped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417b3f00-199f-4db7-b500-b7b7f99ce15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = RandomAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99850e42-8c2b-46a6-9a92-59a0e5940061",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.play(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a6351f5-e532-4703-ae3b-0f7ec2483f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 13, 17, 12, 12, 12, 13, 23, 31, 13, 12, 15]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra.trewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9590104e-899f-4a4a-81a3-0b952a8f1818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.83"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum(ra.trewards) / len(ra.trewards), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568cb0f",
   "metadata": {},
   "source": [
    "**Random Agent Performance Analysis:**\n",
    "\n",
    "The episode lengths show how quickly random trading gets stopped due to poor accuracy. Short episodes indicate:\n",
    "- **Risk control working**: Poor performance triggers early termination\n",
    "- **Market difficulty**: Even random binary prediction struggles\n",
    "- **Baseline established**: Any serious trading algorithm must significantly outperform this\n",
    "\n",
    "**Financial implication**: This simulates what happens to traders who make decisions without any systematic approach - they quickly lose capital and get forced out of the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2252d5e0-0c3f-4900-a96f-1fe6348ccd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2607"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fin.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e651e5-4eb4-4001-b8a3-d629721b6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf97ecf",
   "metadata": {},
   "source": [
    "## Deep Q-Learning for Financial Prediction\n",
    "\n",
    "Now we adapt our CartPole DQN agent for financial markets. The core algorithm remains the same, but we need several modifications for the financial domain:\n",
    "\n",
    "**Key Adaptations for Finance:**\n",
    "1. **Custom environment**: Using our `Finance` class instead of Gymnasium\n",
    "2. **Different reward structure**: Binary accuracy vs. continuous rewards\n",
    "3. **Risk-adjusted gamma**: Lower discount factor (0.5) for shorter-term focus\n",
    "4. **Variable input size**: Flexible `n_features` for different lookback periods\n",
    "5. **Financial stopping criteria**: Performance-based episode termination\n",
    "\n",
    "**Challenges in Financial RL:**\n",
    "- **Sparse rewards**: Correct predictions only known after time passes\n",
    "- **Noisy signals**: Market data contains significant randomness  \n",
    "- **Regime changes**: Market behavior shifts over time\n",
    "- **Overfitting risk**: High-dimensional data with limited samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a04e9dcb-5a0c-463b-9714-012a9b8e4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c047b3c4-d7ca-4e17-b290-6dfce70690fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c5656a5-7378-494b-a43f-5ba736105485",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.legacy.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a1c06c7-6477-4a73-9bf5-68b497c52e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, symbol, feature, min_accuracy, n_features=4):\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9975\n",
    "        self.epsilon_min = 0.1\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.5\n",
    "        self.trewards = list()\n",
    "        self.max_treward = 0\n",
    "        self.n_features = n_features\n",
    "        self._create_model()\n",
    "        self.env = Finance(symbol, feature,\n",
    "                    min_accuracy, n_features)\n",
    "    def _create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, activation='relu',\n",
    "                             input_dim=self.n_features))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=opt)\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, next_state, reward, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(\n",
    "                    self.model.predict(next_state)[0])\n",
    "            target = self.model.predict(state)\n",
    "            target[0, action] = reward\n",
    "            self.model.fit(state, target, epochs=1, verbose=False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.n_features])\n",
    "            for f in range(1, 5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, _ = \\\n",
    "                    self.env.step(action)\n",
    "                next_state = np.reshape(next_state,\n",
    "                                        [1, self.n_features])\n",
    "                self.memory.append(\n",
    "                    [state, action, next_state, reward, done])\n",
    "                state = next_state \n",
    "                if done:\n",
    "                    self.trewards.append(f)\n",
    "                    self.max_treward = max(self.max_treward, f)\n",
    "                    templ = f'episode={e:4d} | treward={f:4d}'\n",
    "                    templ += f' | max={self.max_treward:4d}'\n",
    "                    print(templ, end='\\r')\n",
    "                    break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "        print()\n",
    "    def test(self, episodes):\n",
    "        ma = self.env.min_accuracy\n",
    "        self.env.min_accuracy = 0.5\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.n_features])\n",
    "            for f in range(1, 5001):\n",
    "                action = np.argmax(self.model.predict(state)[0])\n",
    "                state, reward, done, trunc, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, self.n_features])\n",
    "                if done:\n",
    "                    tmpl = f'total reward={f} | '\n",
    "                    tmpl += f'accuracy={self.env.accuracy:.3f}'\n",
    "                    print(tmpl)\n",
    "                    break\n",
    "        self.env.min_accuracy = ma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec246a9",
   "metadata": {},
   "source": [
    "### Financial DQN Agent Implementation\n",
    "\n",
    "**Key Modifications for Financial Markets:**\n",
    "\n",
    "**1. Hyperparameter Adjustments:**\n",
    "- **`gamma = 0.5`**: Lower discount factor for shorter-term predictions (vs. 0.9 in CartPole)\n",
    "- **Flexible input**: `input_dim=n_features` adapts to different lookback periods\n",
    "- **Custom environment**: Integration with our `Finance` class\n",
    "\n",
    "**2. Network Architecture:**\n",
    "- **Input layer**: `n_features` neurons (default 4 for 4-period lookback)\n",
    "- **Hidden layers**: 24 neurons each (may need adjustment for financial complexity)\n",
    "- **Output layer**: 2 neurons (UP/DOWN predictions)\n",
    "\n",
    "**3. Financial-Specific Methods:**\n",
    "- **`learn()`**: Adapted for financial episode structure and termination\n",
    "- **`test()`**: Modified to report accuracy metrics alongside episode performance\n",
    "- **Risk management**: Integration with `min_accuracy` stopping criteria\n",
    "\n",
    "**4. Performance Tracking:**\n",
    "- **Episode rewards**: Number of consecutive correct predictions\n",
    "- **Accuracy metrics**: Real-time calculation of prediction success rate\n",
    "- **Maximum tracking**: Monitoring best performance achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d83cf567-0389-474d-accd-38431edaf755",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(250)\n",
    "tf.random.set_seed(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "268f6f90-082d-4827-bdef-8bffa57016c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQLAgent('EUR=', 'r', 0.495, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df936f5c",
   "metadata": {},
   "source": [
    "### Configuring the Financial Agent\n",
    "\n",
    "**Agent Configuration:**\n",
    "- **Symbol**: 'EUR=' (EUR/USD exchange rate)\n",
    "- **Feature**: 'r' (using returns instead of price levels)\n",
    "- **Min accuracy**: 0.495 (slightly below 50% - allowing for some noise)\n",
    "- **Features**: 4 (using 4 lagged returns as state)\n",
    "\n",
    "**Why these settings?**\n",
    "- **Returns over prices**: More stationary, easier for neural networks to learn\n",
    "- **Lenient accuracy threshold**: Accounts for market noise and gives agent time to learn\n",
    "- **4-period lookback**: Captures short-term momentum without too much complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae2336af-de7e-4b3a-8ecd-292a06a0beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode= 250 | treward=  12 | max=2603\n",
      "CPU times: user 21.1 s, sys: 3.05 s, total: 24.1 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%time agent.learn(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789faf85",
   "metadata": {},
   "source": [
    "### Training the Financial Prediction Agent\n",
    "\n",
    "**Training for 250 episodes on EUR/USD data:**\n",
    "\n",
    "**What to expect:**\n",
    "- **Early episodes**: Short runs due to random predictions and risk controls\n",
    "- **Learning phase**: Gradual improvement as patterns emerge in financial data\n",
    "- **Convergence**: Hopefully longer episodes with >49.5% accuracy\n",
    "\n",
    "**Key differences from CartPole:**\n",
    "- **Variable episode lengths**: Depend on prediction accuracy, not just time\n",
    "- **Market dependency**: Performance varies with market conditions and volatility\n",
    "- **Financial noise**: Success less predictable due to inherent market randomness\n",
    "\n",
    "**Performance indicators:**\n",
    "- **Episode length**: Longer episodes = better prediction accuracy\n",
    "- **Maximum episodes**: Best performance achieved so far\n",
    "- **Consistency**: Stable performance across multiple episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a1023a5-07ef-4ac3-86c4-307a356cd2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward=2603 | accuracy=0.525\n",
      "total reward=2603 | accuracy=0.525\n",
      "total reward=2603 | accuracy=0.525\n",
      "total reward=2603 | accuracy=0.525\n",
      "total reward=2603 | accuracy=0.525\n"
     ]
    }
   ],
   "source": [
    "agent.test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effdb19",
   "metadata": {},
   "source": [
    "### Testing Financial Agent Performance\n",
    "\n",
    "**Testing with pure exploitation (no exploration):**\n",
    "\n",
    "**Metrics to watch:**\n",
    "- **Total reward**: Number of consecutive correct predictions before stopping\n",
    "- **Accuracy**: Final prediction accuracy percentage\n",
    "- **Consistency**: Performance stability across test episodes\n",
    "\n",
    "**Success criteria:**\n",
    "- **Accuracy > 50%**: Better than random guessing\n",
    "- **Longer episodes**: More consecutive correct predictions\n",
    "- **Stable performance**: Consistent results across multiple tests\n",
    "\n",
    "**Real-world implications:**\n",
    "If the agent achieves >55% accuracy consistently, this could translate to profitable trading strategies in real markets (after accounting for transaction costs and slippage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f6a3e",
   "metadata": {},
   "source": [
    "## Summary: From Games to Markets\n",
    "\n",
    "This notebook marked a crucial transition from controlled environments to real-world financial applications:\n",
    "\n",
    "### Key Achievements:\n",
    "1. **Custom Environment Creation**: Built a financial prediction environment from scratch\n",
    "2. **Real Data Integration**: Successfully applied RL to actual EUR/USD market data  \n",
    "3. **Risk Management**: Implemented performance-based stopping criteria\n",
    "4. **Feature Engineering**: Compared price levels vs. returns as state representations\n",
    "\n",
    "### Financial RL Innovations:\n",
    "- **Domain Adaptation**: Modified DQN architecture for financial prediction tasks\n",
    "- **Performance Metrics**: Accuracy-based rewards instead of game scores\n",
    "- **Risk Controls**: Early termination to prevent catastrophic losses\n",
    "- **Data Preprocessing**: Normalization and feature engineering for financial time series\n",
    "\n",
    "### Technical Insights:\n",
    "- **State Representation Matters**: Returns vs. prices can lead to different strategies\n",
    "- **Risk-Reward Balance**: Lower gamma (0.5) focuses on shorter-term predictions\n",
    "- **Market Efficiency**: Even small improvements over random (50%) can be valuable\n",
    "- **Noise Management**: Financial data requires careful handling of random variation\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Algorithmic Trading**: Systematic prediction and execution of trades\n",
    "- **Risk Management**: Early warning systems for portfolio protection\n",
    "- **Market Making**: Predicting short-term price movements for spread capture\n",
    "- **Portfolio Optimization**: Asset allocation based on return predictions\n",
    "\n",
    "### Lessons Learned:\n",
    "- **Markets are Hard**: Financial prediction is more challenging than game environments\n",
    "- **Data Quality Matters**: Preprocessing and feature selection are crucial\n",
    "- **Risk Control Essential**: Stop-loss mechanisms prevent catastrophic failures\n",
    "- **Small Edges Count**: Even 52-53% accuracy can be very valuable in finance\n",
    "\n",
    "### Next Steps:\n",
    "This foundation enables exploration of:\n",
    "- **Multi-asset portfolios**: Trading multiple instruments simultaneously\n",
    "- **Transaction costs**: Including realistic trading costs and slippage\n",
    "- **Position sizing**: Optimizing bet sizes based on confidence levels\n",
    "- **Alternative features**: Technical indicators, sentiment data, news analysis\n",
    "- **Advanced architectures**: LSTMs, Transformers for time-series modeling\n",
    "\n",
    "**The Journey So Far:**\n",
    "- **Notebook 1**: Simple learning with coins and dice\n",
    "- **Notebook 2**: Deep Q-Learning with CartPole  \n",
    "- **Notebook 3**: Real-world application to financial markets\n",
    "\n",
    "We've progressed from basic concepts to practical applications that could actually generate trading signals in live markets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
