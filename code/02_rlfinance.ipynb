{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Finance\n",
    "\n",
    "**Chapter 02 &mdash; Deep Q-Learning**\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be6f8b-e00e-402c-9df1-1d3f16e76c7e",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc06b8",
   "metadata": {},
   "source": [
    "## Introduction to Deep Q-Learning\n",
    "\n",
    "This notebook demonstrates **Deep Q-Learning (DQN)**, a breakthrough algorithm that combines Q-learning with deep neural networks. We'll use the classic **CartPole** environment to show how an agent can learn complex control tasks.\n",
    "\n",
    "**Key Concepts We'll Explore:**\n",
    "- **Q-Learning**: Learning action-value functions that estimate future rewards\n",
    "- **Deep Neural Networks**: Using neural networks to approximate Q-functions for complex state spaces\n",
    "- **Experience Replay**: Storing and reusing past experiences to improve learning stability\n",
    "- **Epsilon-Greedy Exploration**: Balancing exploration of new actions with exploitation of known good actions\n",
    "- **Target Networks**: Techniques to stabilize training in deep reinforcement learning\n",
    "\n",
    "**Why CartPole?**\n",
    "CartPole is a classic control problem where an agent must balance a pole on a cart by moving the cart left or right. It's an excellent testbed for RL algorithms because:\n",
    "- The state space is continuous (position, velocity, angle, angular velocity)\n",
    "- The action space is discrete (left or right)\n",
    "- Success requires learning long-term consequences of actions\n",
    "- It's simple enough to understand but complex enough to require sophisticated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3924c3-2cad-4400-8806-5acf2f4b9b16",
   "metadata": {},
   "source": [
    "### The Game Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f3a51a-71e6-497d-bab3-926444a6bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c284e6",
   "metadata": {},
   "source": [
    "**Gymnasium** (formerly OpenAI Gym) is the standard toolkit for developing and comparing reinforcement learning algorithms. It provides a wide variety of environments with standardized interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19725f2-a026-487e-826c-00fa5fce71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c31d2f",
   "metadata": {},
   "source": [
    "### Creating the CartPole Environment\n",
    "\n",
    "`CartPole-v1` is a classic control task where:\n",
    "- **Goal**: Keep a pole balanced upright on a movable cart\n",
    "- **Actions**: Move cart left (0) or right (1)  \n",
    "- **Episode ends**: When pole falls too far (>15°) or cart moves too far (>2.4 units)\n",
    "- **Success**: Keeping the pole upright for 500 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af76fb4e-3b31-4465-bff5-e5f8362af3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb45da1-6f9c-464d-bb16-e098ddd52838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887b08c",
   "metadata": {},
   "source": [
    "### Understanding the Action Space\n",
    "\n",
    "The **action space** defines what actions the agent can take:\n",
    "- **Discrete(2)**: Two possible actions (0 and 1)\n",
    "- **Action 0**: Push cart to the left\n",
    "- **Action 1**: Push cart to the right\n",
    "\n",
    "The `action_space.n` gives us the number of possible actions, and `action_space.sample()` randomly selects an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e8ec50-f5a4-4706-8937-6724582ebdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[env.action_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592d3ddc-3958-42ff-b4c7-8924ce0a343d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19474f1a-29c3-4cc2-89f6-6226845f5468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffad59",
   "metadata": {},
   "source": [
    "### Understanding the Observation Space\n",
    "\n",
    "The **observation space** defines what information the agent receives about the environment state:\n",
    "- **Box(4,)**: A 4-dimensional continuous vector\n",
    "- **State components**:\n",
    "  1. **Cart Position**: Horizontal position of cart (-2.4 to 2.4)\n",
    "  2. **Cart Velocity**: Speed of cart movement\n",
    "  3. **Pole Angle**: Angle of pole from vertical (-0.2095 to 0.2095 radians ≈ ±12°)  \n",
    "  4. **Pole Angular Velocity**: Rate of change of pole angle\n",
    "\n",
    "This 4D state space is what makes CartPole challenging - the agent must learn to coordinate multiple continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bdd054d-4a5e-429e-9e44-3e436a20446d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03349816,  0.0096554 , -0.02111368, -0.04570484], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(seed=100)\n",
    "# cart position, cart velocity, pole angle, pole angular velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875c67b7-4817-4fac-8fbb-0596c399af96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03369127, -0.18515752, -0.02202777,  0.24024247], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19703662",
   "metadata": {},
   "source": [
    "### Environment Interaction\n",
    "\n",
    "Let's see how agent-environment interaction works:\n",
    "- `env.reset()` initializes a new episode and returns the initial state\n",
    "- `env.step(action)` executes an action and returns:\n",
    "  - **next_state**: New observation after the action\n",
    "  - **reward**: Immediate reward (1.0 for each step the pole stays up)\n",
    "  - **done**: Whether episode ended (pole fell or cart went too far)\n",
    "  - **truncated**: Whether episode was truncated (time limit reached)\n",
    "  - **info**: Additional information (usually empty for CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be7afb1-e69d-41d7-b869-c73747e38b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02998812,  0.01027205, -0.01722292, -0.05930644], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f6e49b-3308-418a-999c-f7d6a052cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "    def play(self, episodes=1):\n",
    "        self.trewards = list()\n",
    "        for e in range(episodes):\n",
    "            self.env.reset()\n",
    "            for step in range(1, 100):\n",
    "                a = self.env.action_space.sample()\n",
    "                state, reward, done, trunc, info = self.env.step(a)\n",
    "                if done:\n",
    "                    self.trewards.append(step)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd55fa",
   "metadata": {},
   "source": [
    "### Baseline: Random Agent\n",
    "\n",
    "Before implementing sophisticated learning algorithms, let's establish a baseline with a **random agent** that chooses actions randomly. This helps us understand:\n",
    "1. How difficult the task is\n",
    "2. What performance we need to beat\n",
    "3. The natural variance in episode lengths\n",
    "\n",
    "The `RandomAgent` class:\n",
    "- Takes random actions using `env.action_space.sample()`\n",
    "- Tracks episode lengths (how long the pole stays up)\n",
    "- Provides a performance baseline for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dffbb689-b81e-48cc-9fac-3a7dec9c1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = RandomAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb3b03c-ded1-4ca7-80d2-e316635379b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.play(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b83a7c9-485a-433d-b637-9ffbe6fe7146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 17, 10, 13, 13, 12, 35, 21, 17, 26, 16, 49, 20, 19, 26]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra.trewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d9d910-4f2d-4d7b-bcaa-a28747474c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.87"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum(ra.trewards) / len(ra.trewards), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f10bb4",
   "metadata": {},
   "source": [
    "**Random Agent Performance**: The random agent typically achieves episode lengths of 10-30 steps on average. This poor performance shows that CartPole requires intelligent action selection - random movements quickly lead to the pole falling over.\n",
    "\n",
    "**Our Goal**: Develop a learning agent that can consistently achieve the maximum episode length of 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e1594d-ea7c-49e9-9149-92848ba72440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed0c5d",
   "metadata": {},
   "source": [
    "## Deep Q-Learning (DQN) Implementation\n",
    "\n",
    "Now we'll implement a **Deep Q-Network (DQN)** agent that can learn to solve CartPole. DQN was introduced by DeepMind in 2015 and represents a major breakthrough in reinforcement learning.\n",
    "\n",
    "### Key Components:\n",
    "1. **Neural Network**: Approximates the Q-function Q(state, action)\n",
    "2. **Experience Replay**: Stores experiences and replays them for learning\n",
    "3. **Epsilon-Greedy**: Balances exploration vs exploitation\n",
    "4. **Target Network**: Stabilizes learning (we'll use a simplified version)\n",
    "\n",
    "### Required Libraries:\n",
    "- **TensorFlow/Keras**: For building and training neural networks\n",
    "- **NumPy**: For numerical computations\n",
    "- **Collections.deque**: For efficient experience replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa105bbb-727f-488d-8152-b5c1cc4d7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a21cd6c5-058b-45cb-abfa-78a9cbb3633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0264fac6-2c4a-4ea3-9031-e5006dce93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.legacy.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7c28ee7-4be2-459c-8e27-029ec6ff4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "tf.random.set_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361f496",
   "metadata": {},
   "source": [
    "### Environment Setup and Reproducibility\n",
    "\n",
    "These setup steps ensure:\n",
    "- **Reproducible results**: Fixed random seeds for consistent learning curves\n",
    "- **Clean output**: Suppressed TensorFlow warnings and logs\n",
    "- **Stable training**: Disabled eager execution for compatibility with older TensorFlow patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "072e8f75-0936-434f-ad65-c2f7cff91b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9975\n",
    "        self.epsilon_min = 0.1\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.9\n",
    "        self.trewards = list()\n",
    "        self.max_treward = 0\n",
    "        self._create_model()\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "    def _create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, activation='relu', input_dim=4))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dcacb",
   "metadata": {},
   "source": [
    "### DQN Agent Architecture\n",
    "\n",
    "The `DQLAgent` class implements the core Deep Q-Learning algorithm:\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **epsilon**: Exploration rate (starts at 1.0 = 100% random)\n",
    "- **epsilon_decay**: Rate of reducing exploration over time (0.9975)\n",
    "- **epsilon_min**: Minimum exploration rate (0.1 = 10% random actions)\n",
    "- **memory**: Experience replay buffer (stores last 2000 experiences)\n",
    "- **batch_size**: Number of experiences to sample for training (32)\n",
    "- **gamma**: Discount factor for future rewards (0.9)\n",
    "\n",
    "**Neural Network Architecture:**\n",
    "- **Input layer**: 4 neurons (for the 4-dimensional state)\n",
    "- **Hidden layers**: 2 layers with 24 neurons each (ReLU activation)\n",
    "- **Output layer**: 2 neurons (Q-values for each action)\n",
    "- **Loss function**: Mean Squared Error (MSE)\n",
    "- **Optimizer**: Adam with learning rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03e2299c-14bd-4cc8-af41-89b69d532544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent(DQLAgent):\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, next_state, reward, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(\n",
    "                    self.model.predict(next_state)[0])\n",
    "            target = self.model.predict(state)\n",
    "            target[0, action] = reward\n",
    "            self.model.fit(state, target, epochs=2, verbose=False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62667c",
   "metadata": {},
   "source": [
    "### Action Selection and Experience Replay\n",
    "\n",
    "**The `act()` method implements epsilon-greedy action selection:**\n",
    "- With probability `epsilon`: Choose a random action (exploration)\n",
    "- With probability `1-epsilon`: Choose the action with highest Q-value (exploitation)\n",
    "- Over time, epsilon decreases, shifting from exploration to exploitation\n",
    "\n",
    "**The `replay()` method implements experience replay:**\n",
    "1. **Sample**: Randomly select a batch of past experiences from memory\n",
    "2. **Compute targets**: For each experience, calculate the target Q-value:\n",
    "   - If episode ended: target = immediate reward\n",
    "   - If episode continues: target = reward + gamma * max(Q(next_state))\n",
    "3. **Train**: Update the neural network to minimize prediction error\n",
    "4. **Decay epsilon**: Gradually reduce exploration rate\n",
    "\n",
    "This approach breaks the correlation between consecutive experiences and stabilizes learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bf59f89-41a4-4f6e-8635-0513b3c3d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent(DQLAgent):\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, 4])\n",
    "            for f in range(1, 5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, _ = \\\n",
    "                    self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, 4])\n",
    "                self.memory.append(\n",
    "                    [state, action, next_state, reward, done])\n",
    "                state = next_state\n",
    "                if done or trunc:\n",
    "                    self.trewards.append(f)\n",
    "                    self.max_treward = max(self.max_treward, f)\n",
    "                    templ = f'episode={e:4d} | treward={f:4d}'\n",
    "                    templ += f' | max={self.max_treward:4d}'\n",
    "                    print(templ, end='\\r')\n",
    "                    break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cc21c",
   "metadata": {},
   "source": [
    "### The Learning Process\n",
    "\n",
    "**The `learn()` method orchestrates the training:**\n",
    "\n",
    "**For each episode:**\n",
    "1. **Reset environment** and get initial state\n",
    "2. **Reshape state** to match neural network input format [1, 4]\n",
    "3. **Play episode** for up to 5000 steps:\n",
    "   - Choose action using epsilon-greedy policy\n",
    "   - Execute action and observe results\n",
    "   - Store experience (state, action, next_state, reward, done) in memory\n",
    "   - Move to next state\n",
    "4. **Track performance** by recording episode length\n",
    "5. **Learn from experience** by calling replay() if enough experiences stored\n",
    "\n",
    "**Key Features:**\n",
    "- **Early termination**: Episode ends when pole falls or cart moves too far\n",
    "- **Progress tracking**: Displays current episode, reward, and maximum achieved\n",
    "- **Experience replay**: Only starts learning after collecting enough experiences\n",
    "- **Continuous learning**: Neural network updates after every episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a44a5f9-af9b-4929-a5c4-19e87f871c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent(DQLAgent):\n",
    "    def test(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, 4])\n",
    "            for f in range(1, 5001):\n",
    "                action = np.argmax(self.model.predict(state)[0])\n",
    "                state, reward, done, trunc, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, 4])\n",
    "                if done or trunc:\n",
    "                    print(f, end=' ')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7201a6",
   "metadata": {},
   "source": [
    "### Testing the Trained Agent\n",
    "\n",
    "**The `test()` method evaluates the trained agent:**\n",
    "- **Pure exploitation**: Always chooses the action with highest Q-value (no exploration)\n",
    "- **No learning**: The neural network weights are frozen during testing\n",
    "- **Performance measurement**: Records episode lengths to assess learned policy quality\n",
    "\n",
    "This gives us a clean measure of how well the agent has learned the task without the noise of exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64417ca0-49ba-4558-8c92-d89604ff3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQLAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f77a72ab-5a4b-4d3d-863a-f8d08d2e3ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=1500 | treward= 254 | max= 500\n",
      "CPU times: user 2min 11s, sys: 23.2 s, total: 2min 34s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%time agent.learn(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4615d",
   "metadata": {},
   "source": [
    "### Training the DQN Agent\n",
    "\n",
    "Now we train the agent for 1500 episodes. Watch the progress:\n",
    "\n",
    "**What to expect during training:**\n",
    "- **Early episodes**: Short episode lengths (similar to random agent) due to high exploration\n",
    "- **Learning phase**: Gradual improvement as the agent discovers effective strategies\n",
    "- **Convergence**: Eventually achieving consistent 500-step episodes (perfect performance)\n",
    "\n",
    "**Training dynamics:**\n",
    "- **Exploration decreases**: Epsilon decays from 1.0 to 0.1 over time\n",
    "- **Experience accumulates**: Memory buffer fills with diverse experiences\n",
    "- **Q-function improves**: Neural network learns better action-value estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbfc1255-66fe-4c69-9135-70100b981109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09997053357470892"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb7e23",
   "metadata": {},
   "source": [
    "### Final Exploration Rate\n",
    "\n",
    "After training, let's check the final epsilon value. This shows how much exploration vs exploitation the agent will use in future episodes. A value around 0.1 means the agent will still explore 10% of the time to avoid getting stuck in local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af72f8d3-4e2a-4d0f-8311-a56ba4487832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 211 206 101 198 234 115 287 241 116 98 201 120 174 95 "
     ]
    }
   ],
   "source": [
    "agent.test(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3e91f",
   "metadata": {},
   "source": [
    "### Testing Trained Agent Performance\n",
    "\n",
    "Now let's test the trained agent's performance with pure exploitation (no exploration). Each number represents how many steps the agent kept the pole balanced in that episode.\n",
    "\n",
    "**Expected results:**\n",
    "- **Well-trained agent**: Should consistently achieve 500 steps (maximum possible)\n",
    "- **Comparison to random agent**: Remember the random agent averaged ~15-25 steps\n",
    "- **Success metric**: Episodes reaching 500 steps indicate the agent has mastered the task\n",
    "\n",
    "If the agent consistently achieves 500 steps, it has successfully learned the optimal policy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58340b22",
   "metadata": {},
   "source": [
    "## Summary: From Simple Learning to Deep Q-Learning\n",
    "\n",
    "This notebook demonstrated a major leap in reinforcement learning sophistication:\n",
    "\n",
    "### Key Achievements:\n",
    "1. **Environment mastery**: Learned to solve a continuous control problem with discrete actions\n",
    "2. **Performance improvement**: From ~20 steps (random) to 500 steps (optimal)\n",
    "3. **Stable learning**: Used experience replay and epsilon-greedy exploration for robust training\n",
    "\n",
    "### Deep Q-Learning Innovations:\n",
    "- **Function approximation**: Neural networks can handle continuous state spaces\n",
    "- **Experience replay**: Breaking temporal correlations improves learning stability  \n",
    "- **Exploration-exploitation balance**: Epsilon-greedy provides systematic exploration\n",
    "- **Scalability**: Same approach works for much more complex environments\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Finance**: Portfolio optimization, algorithmic trading, risk management\n",
    "- **Robotics**: Robot control, manipulation, navigation\n",
    "- **Games**: Game playing (AlphaGo, StarCraft, Dota)\n",
    "- **Autonomous systems**: Self-driving cars, drones, smart grids\n",
    "\n",
    "### Next Steps:\n",
    "This foundation enables exploration of:\n",
    "- **Double DQN**: Addressing overestimation bias\n",
    "- **Dueling DQN**: Separating state values from action advantages  \n",
    "- **Policy Gradient methods**: Direct policy optimization\n",
    "- **Actor-Critic algorithms**: Combining value and policy learning\n",
    "- **Financial applications**: Applying these techniques to trading and portfolio management\n",
    "\n",
    "The transition from simple coin-flipping (Notebook 1) to complex control tasks (this notebook) showcases the power and potential of reinforcement learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
