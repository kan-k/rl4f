{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Finance\n",
    "\n",
    "**Chapter 02 &mdash; Deep Q-Learning**\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be6f8b-e00e-402c-9df1-1d3f16e76c7e",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3924c3-2cad-4400-8806-5acf2f4b9b16",
   "metadata": {},
   "source": [
    "### The Game Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3a51a-71e6-497d-bab3-926444a6bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19725f2-a026-487e-826c-00fa5fce71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76fb4e-3b31-4465-bff5-e5f8362af3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb45da1-6f9c-464d-bb16-e098ddd52838",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8ec50-f5a4-4706-8937-6724582ebdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[env.action_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d3ddc-3958-42ff-b4c7-8924ce0a343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19474f1a-29c3-4cc2-89f6-6226845f5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd054d-4a5e-429e-9e44-3e436a20446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(seed=100)\n",
    "# cart position, cart velocity, pole angle, pole angular velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c67b7-4817-4fac-8fbb-0596c399af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7afb1-e69d-41d7-b869-c73747e38b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6e49b-3308-418a-999c-f7d6a052cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "    def play(self, episodes=1):\n",
    "        self.trewards = list()\n",
    "        for e in range(episodes):\n",
    "            self.env.reset()\n",
    "            for step in range(1, 100):\n",
    "                a = self.env.action_space.sample()\n",
    "                state, reward, done, trunc, info = self.env.step(a)\n",
    "                if done:\n",
    "                    self.trewards.append(step)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbb689-b81e-48cc-9fac-3a7dec9c1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = RandomAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3b03c-ded1-4ca7-80d2-e316635379b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.play(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83a7c9-485a-433d-b637-9ffbe6fe7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.trewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9d910-4f2d-4d7b-bcaa-a28747474c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(sum(ra.trewards) / len(ra.trewards), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1594d-ea7c-49e9-9149-92848ba72440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa105bbb-727f-488d-8152-b5c1cc4d7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264fac6-2c4a-4ea3-9031-e5006dce93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c28ee7-4be2-459c-8e27-029ec6ff4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "np.random.seed(100)\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e8f75-0936-434f-ad65-c2f7cff91b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9975\n",
    "        self.epsilon_min = 0.1\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.9\n",
    "        self.trewards = []\n",
    "        self.max_treward = 0\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self._create_model().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    def _create_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, self.action_size)\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2299c-14bd-4cc8-af41-89b69d532544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent(DQLAgent):\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, next_states, rewards, dones = zip(*batch)\n",
    "        states = torch.FloatTensor(states).to(self.device).squeeze(1)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device).squeeze(1)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        q_values = self.model(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_value = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        loss = self.criterion(q_value, expected_q_value.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf59f89-41a4-4f6e-8635-0513b3c3d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent(DQLAgent):\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            for f in range(1, 5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                self.memory.append([state, action, next_state, reward, done])\n",
    "                state = next_state\n",
    "                if done or trunc:\n",
    "                    self.trewards.append(f)\n",
    "                    self.max_treward = max(self.max_treward, f)\n",
    "                    templ = f'episode={e:4d} | treward={f:4d}'\n",
    "                    templ += f' | max={self.max_treward:4d}'\n",
    "                    print(templ, end='\\r')\n",
    "                    break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44a5f9-af9b-4929-a5c4-19e87f871c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent(DQLAgent):\n",
    "    def test(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            for f in range(1, 5001):\n",
    "                state_tensor = torch.FloatTensor(state).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.model(state_tensor)\n",
    "                action = torch.argmax(q_values).item()\n",
    "                state, reward, done, trunc, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "                if done or trunc:\n",
    "                    print(f, end=' ')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64417ca0-49ba-4558-8c92-d89604ff3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQLAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a72ab-5a4b-4d3d-863a-f8d08d2e3ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time agent.learn(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc1255-66fe-4c69-9135-70100b981109",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72f8d3-4e2a-4d0f-8311-a56ba4487832",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
    "\n",
    "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
